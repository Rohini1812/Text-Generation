#importing libraries
import numpy
import sys
import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from tensorflow import keras
from keras.models import sequential
from keras.layers import Dense,Dropout,LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint

#Loading data
file=open("E:\\frankenstein.txt").read() 

#Tokenize words from input data
def tokenize_words(input):
    input=input.lower()
    tokenizer=RegexpTokenizer(r'\w')
    token=tokenizer.tokenize(input)
    filtered=filter(lambda token : token not in stopwords.words('english'),token)
    return "".join(filtered)
    
processed_inputs=tokenize_words(file)

#converting characters to numbers
chars=sorted(list(set(processed_inputs)))
char_to_num=dict((c,i) for i,c in enumerate(chars))

#checking wether teh conversion has worked
input_len=len(processed_inputs)
vocab_len=len(chars)
print("Total no of characters : ",input_len)
print("Total no of vocab : ",vocab_len)

#sequence length
seq_length=100
x_data=[]
y_data=[]

#loop through the sequence
for i in range (0, seq_length,1):
    in_seq=processed_inputs[i:i+seq_length]
    out_seq=processed_inputs[i+seq_length]
    x_data.append([char_to_num[char] for char in in_seq])
    y_data.append(char_to_num[out_seq])

n_patterns=len(x_data)
print("Total Patterns : ",n_patterns)

#converting input sequence into np array and so on
x=numpy.reshape(x_data,(n_patterns,seq_length,1))
x=x/float(vocab_len)

#one-hot encoding
y=np_utils.to_categorical(y_data)

#creating model
model=Sequential()
model.add(LSTM(256,input_shape = ( x.shape[1],x.shape[2] ),return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1],activation='softmax'))

#compiling the model
model.compile(loss="categorical_crossentropy",optimizer='adam')

#saving weights
filepath="model_weights_saved"
checkpoint=ModelCheckpoint(filepath,monitor="loss",verbose=1,save_best_only=True,mode="min")
desired_callbacks=[checkpoint]

#Fitting & training the model
model.fit(x,y, epochs=4, batch_size=256, callbacks=desired_callbacks)

#Recompiling the model with saved weights
filename="model_weights_saved"
model.load_weights(filename)
model.compile(loss="categorical_crossentropy",optimizer="adam")

#converting output back into characters
num_to_char=dict((i,c) for i,c in enumerate(chars))

#random seed to help generation process
start=numpy.random.randint(0,len(x.data)-1)
pattern=x_data[start]
print("Random seed :")
print("\"", ''.join([num_to_char[value] for value in pattern]), "\"")

#Generate the test
for i in range(1000):
    x=numpy.reshape(pattern,(1,len(pattern),1))
    x=x/float(vocab_len)
    prediction=model.predict(x,verbose=0)
    index=numpy.argmax(prediction)
    result=num_to_char[index]
    seq_in=[num_to_char[value] for value in pattern]
    sys.stdout.write(result)
    pattern.append(index)
    pattern=pattern[1: len(pattern)]
